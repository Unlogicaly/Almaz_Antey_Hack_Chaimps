{
 "cells": [
  {
   "metadata": {
    "id": "WTKBgShBb7Ov",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pickle\n",
    "import torchvision\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from pathlib import Path\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import colors, pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image as Image\n",
    "import PIL\n",
    "import time\n",
    "import cv2\n",
    "from scipy.interpolate import make_interp_spline, BSpline, interp1d\n",
    "import operator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import os\n",
    "from torch.utils.data import DataLoader, RandomSampler, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ToPIL = transforms.ToPILImage()"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.is_available()"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 2,
     "data": {
      "text/plain": "True"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {
    "id": "jbFWiZf6g-NG",
    "outputId": "e35c0d0d-e478-441b-d422-6518bc89d105",
    "trusted": true
   },
   "cell_type": "code",
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "WtLtN05zb8L-",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class ShipsDataset(Dataset):\n",
    "    def __init__(self, files, mode, transform, csv_file = None):\n",
    "        assert mode in ['train', 'test', 'val']\n",
    "        \n",
    "        self.files = files\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.len_ = len(self.files)\n",
    "        self.filters = [None]\n",
    "        if (mode != 'test'):\n",
    "            self.labels = [csv_file.loc[str(file).split('/')[-1]]['IdCls'] if type(csv_file.loc[str(file).split('/')[-1]]['IdCls']) == np.int64 else 1 for file in files]\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            self.label_encoder.fit(self.labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "    \n",
    "    def load_img(self, fname):\n",
    "        fname = str(fname)\n",
    "        img = Image.open(fname)\n",
    "        img = np.array(Image.open(fname))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.blur(img, (11,11))\n",
    "        img = cv2.Laplacian(img, cv2.THRESH_BINARY, scale=0.55, ksize=5)\n",
    "        img = Image.fromarray(np.uint8(img))\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.load_img(fname=self.files[idx])\n",
    "        x = self.transform(x)\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            return x\n",
    "        else:\n",
    "            y = self.labels[idx]\n",
    "            y = self.label_encoder.transform([y]).item()\n",
    "            return x, y"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "id": "IW-iLfBCb_Fd",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def image_dims(source):\n",
    "    \"\"\"\n",
    "    Получение размеров входных изображений\n",
    "    :param source (DataLoader, ndarray, Tensor, Jpeg): Данные для извлечения размера изображения\n",
    "    :return: Размеры изображения (tuple)\n",
    "    \"\"\"\n",
    "    if type(source) is torch.utils.data.dataloader.DataLoader:\n",
    "        image, _ = iter(source).next()\n",
    "        return (image.shape[-2], image.shape[-1])\n",
    "    if type(source) is PIL.JpegImagePlugin.JpegImageFile:\n",
    "        source = np.array(source)\n",
    "    if type(source) is np.ndarray or type(source) is torch.Tensor:\n",
    "        shapes = source.shape[-3:]\n",
    "        if shapes[2] == 3 or shapes[2] == 1:\n",
    "            return (shapes[0], shapes[1])\n",
    "        else:\n",
    "            return (shapes[1], shapes[2])\n",
    "    raise Exception('Unknown type of file')\n"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "id": "v6N58tXMWRw4"
   },
   "cell_type": "markdown",
   "source": [
    "Данные функции нужны для работы с файлами 'Losses.txt' и 'CNNs.txt'. Они имеют строгую структуру и нужны для сравнения различных нейросетей"
   ]
  },
  {
   "metadata": {
    "id": "Hwe6yxd4cC48",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def get_loss(path, n):\n",
    "    \"\"\"\n",
    "    Преобразование файла с losses в переменную (для построения графиков)\n",
    "    :param path (string): Путь к файлу, из которого извлекаются данные\n",
    "    :param n (string, int): Номер нейронной сети для извлечения\n",
    "    :return: Losses данной нейронной сети (list)\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as file:\n",
    "        t = list(filter(bool, file.read().split('&')))\n",
    "        for i in t:\n",
    "            if int(i.split('\\n')[0]) == int(n):\n",
    "                t = i.split('\\n')[1]\n",
    "                break\n",
    "        try:\n",
    "            t = t.split()\n",
    "        except:\n",
    "            raise Exception('Ошибка в losses {}. Невозможно получить данные'.format(n))\n",
    "        t[0] = t[0][1:]\n",
    "        t.pop(-1)\n",
    "        return list(map(lambda x: np.round(float(x), 3), t))\n",
    "\n",
    "\n",
    "def get_all_losses(path):\n",
    "    \"\"\"\n",
    "    \"Достаёт\" все losses из файла\n",
    "    :param path (string): Путь к файлу для извлечения losses\n",
    "    :return: Все losses из файла (dict)\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as file:\n",
    "        keys = []\n",
    "        for i in list(filter(bool, file.read().split('&'))):\n",
    "            keys.append(int(i.split('\\n')[0]))\n",
    "        losses = [get_loss(path, i) for i in keys]\n",
    "        return {keys[i]: losses[i] for i in range(len(keys))}\n",
    "\n",
    "\n",
    "def get_losses(path, keys):\n",
    "    \"\"\"\n",
    "    \"Достаёт\" losses указанных сетей из файла\n",
    "    :param path (string): Путь к файлу\n",
    "    :param keys (list, tuple, ndarray): Номера сетей losses которых надо извлечь\n",
    "    :return: Losses указанных сетей (dict)\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as file:\n",
    "        losses = [get_loss(path, i) for i in sorted(keys)]\n",
    "        return {sorted(keys)[i]: losses[i] for i in range(len(keys))}\n",
    "\n"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "id": "NwWNNlzVcFuG",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def check_size_losses(path, size=124, logging=True):\n",
    "    \"\"\"\n",
    "    Проверка размеров losses в указанном файле\n",
    "    :param path (string): Путь к файлу\n",
    "    :param size (int): Предпологаемый размер каждого лосса\n",
    "    :param logging (bool): Флаг вывода изменений в файле\n",
    "    :return: Losses, соответствующие предпологаемому размеру (dict). Флаг наличия изменений в файле\n",
    "    \"\"\"\n",
    "    losses = get_all_losses(path)\n",
    "    tmp = []\n",
    "    changed = False\n",
    "    for i in losses.keys():\n",
    "        if len(losses[i]) < size:\n",
    "            if logging:\n",
    "                print('Длина losses сети {} меньше обозначенной. Losses данной сети стёрты'.format(i))\n",
    "            changed = True\n",
    "            continue\n",
    "        if len(losses[i]) > size:\n",
    "            if logging:\n",
    "                print('Длина losses сети {} больше обозначенной. Losses сокращены до {}'.format(i, size))\n",
    "            changed = True\n",
    "            tmp.append([i, losses[i][:size]])\n",
    "            continue\n",
    "        tmp.append([i, losses[i]])\n",
    "    return {tmp[i][0]: tmp[i][1] for i in range(len(tmp))}, changed\n",
    "\n",
    "\n",
    "def standard_losses(path='Losses.txt', size=124, logging=True):\n",
    "    \"\"\"\n",
    "    Приводит файл с losses к стандартному виду (нумерация с единицы, размеры приведены к size)\n",
    "    :param path (string): Путь к файлу\n",
    "    :param size (int): Предпологаемый размер всех losses\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    old_losses = get_all_losses(path)\n",
    "    losses, changed = check_size_losses(path, size, logging)\n",
    "    tmp = ''\n",
    "    if changed:\n",
    "        with open(path, 'r') as file:\n",
    "            tmp = file.read()\n",
    "        with open('Old_{}'.format(path), 'w') as file:\n",
    "            file.write(tmp)\n",
    "\n",
    "        tmp = ''\n",
    "        print('\\n')\n",
    "        for i, j in enumerate(losses.keys()):\n",
    "            tmp += '&' + str(i + 1) + '\\n['\n",
    "            for num in losses[j]:\n",
    "                tmp += str(num) + ' '\n",
    "            tmp += ']\\n'\n",
    "            if j != i + 1:\n",
    "                print('{} --> {}'.format(j, i + 1))\n",
    "        with open(path, 'w') as file:\n",
    "            file.write(tmp)\n",
    "\n",
    "        print('\\n\\nФайл {} перезаписан\\nСтарая версия сохранена в файл Old_{}'.format(path, path))\n",
    "    else:\n",
    "        for i, j in enumerate(losses.keys()):\n",
    "            tmp += '&' + str(i + 1) + '\\n['\n",
    "            for num in losses[j]:\n",
    "                tmp += str(num) + ' '\n",
    "            tmp += ']\\n'\n",
    "            if j != i + 1:\n",
    "                print('{} --> {}'.format(j, i + 1))\n",
    "        with open(path, 'w') as file:\n",
    "            file.write(tmp)\n",
    "        print('\\n\\nФайл {} перезаписан'.format(path))\n",
    "\n",
    "\n",
    "def merge_losses(path1, path2, size=124):\n",
    "    \"\"\"\n",
    "    Объединият два файла с losses в один файл стандартного вида\n",
    "    :param path1 (string): Путь к первому файлу\n",
    "    :param path2 (string): Путь ко второму файлу\n",
    "    :param size (int): Предпологаемый размер всех losses\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if check_size_losses(path1, size, False)[1]:\n",
    "        print('Файл {} не приведён к стандартному виду. Провести стандартизацию?'.format(path1))\n",
    "        if str(input()) != '0':\n",
    "            print('\\n{}:\\n'.format(path1))\n",
    "            standard_losses(path1, size)\n",
    "            print('-----------')\n",
    "        else:\n",
    "            raise Exception('Невозможно объединить файлы нестандартного вида')\n",
    "\n",
    "    if check_size_losses(path2, size, False)[1]:\n",
    "        print('Файл {} не приведён к стандартному виду. Провести стандартизацию?'.format(path2))\n",
    "        if str(input()) != '0':\n",
    "            print('\\n{}:\\n'.format(path2))\n",
    "            standard_losses(path2, size)\n",
    "            print('-----------')\n",
    "        else:\n",
    "            raise Exception('Невозможно объединить файлы нестандартного вида')\n",
    "\n",
    "    losses1 = get_all_losses(path1)\n",
    "    losses2 = get_all_losses(path2)\n",
    "    print('\\nMerging:\\nНомера losses в фале {} не изменились\\nНомера losses в файле {} перешли в следующие:\\n'.format(\n",
    "        path1, path2))\n",
    "    for key, loss in losses2.items():\n",
    "        if loss not in losses1.values():\n",
    "            losses1[list(losses1.keys())[-1] + 1] = loss\n",
    "            print('{} --> {}'.format(key, list(losses1.keys())[-1]))\n",
    "    with open('Losses.txt', 'w') as file:\n",
    "        for i, loss in enumerate(losses1.values()):\n",
    "            file.write('&' + str(i + 1) + '\\n[')\n",
    "            for l in loss:\n",
    "                file.write(str(l) + ' ')\n",
    "            file.write(']\\n')\n",
    "\n"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "id": "2VeNGyNfcILp",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def check_standard_cnns(path):\n",
    "    \"\"\"\n",
    "    Проверка файла с нейросетями на стандартный вид\n",
    "    :param path (string): Путь к файлу\n",
    "    :return: Флаг приведённости к стандартному виду\n",
    "    \"\"\"\n",
    "    tmp = ''\n",
    "    with open(path, 'r') as file:\n",
    "        tmp = list(filter(bool, file.read().split('--')))\n",
    "        for i in range(len(tmp)):\n",
    "            tmp[i] = tmp[i].split('ConvNetwork')\n",
    "    keys = [int(''.join(list(filter(lambda x: 48 <= ord(x) <= 58, tmp[i][0])))) for i in range(len(tmp))]\n",
    "    if keys == list(range(1, len(tmp) + 1)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def standard_cnns(path='CNNs.txt'):\n",
    "    \"\"\"\n",
    "    Приводит файл с нейросетями к стандартному виду (нумерация с единицы)\n",
    "    :param path (string): Путь к файлу\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    tmp = []\n",
    "    tmp1 = ''\n",
    "    with open(path, 'r') as file:\n",
    "        tmp1 = file.read()\n",
    "    with open(path, 'r') as file:\n",
    "        tmp = list(filter(bool, file.read().split('--')))\n",
    "        for i in range(len(tmp)):\n",
    "            tmp[i] = tmp[i].split('ConvNetwork')\n",
    "    with open('Old_{}'.format(path), 'w') as file:\n",
    "        file.write(tmp1)\n",
    "\n",
    "    with open(path, 'w') as file:\n",
    "        for i in range(len(tmp)):\n",
    "            num = int(''.join(list(filter(lambda x: 48 <= ord(x) <= 58, tmp[i][0]))))\n",
    "            if num != i + 1:\n",
    "                print('{} --> {}'.format(num, i + 1))\n",
    "            file.write('-----------------------\\n' + str(i + 1) + '\\nConvNetwork' + tmp[i][1])\n",
    "    print('Файл {} перезаписан. Старая версия сохранена в файл Old_{}'.format(path, path))\n",
    "\n",
    "\n",
    "def merge_cnns(path1, path2):\n",
    "    \"\"\"\n",
    "    Объединяет два файла с нейросетями в один файл стандартного вида\n",
    "    :param path1 (string): Путь к первому файлу\n",
    "    :param path2 (string): Путь ко второму файлу\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    tmp1 = []\n",
    "    tmp2 = []\n",
    "    if not check_standard_cnns(path1):\n",
    "        print('Файл {} не приведён к стандартному виду. Провести стандартизацию?'.format(path1))\n",
    "        if str(input()) != '0':\n",
    "            print('\\n{}:\\n'.format(path1))\n",
    "            standard_cnns(path1)\n",
    "            print('-----------')\n",
    "        else:\n",
    "            raise Exception('Невозможно объединить файлы нестандартного вида')\n",
    "\n",
    "    if not check_standard_cnns(path2):\n",
    "        print('Файл {} не приведён к стандартному виду. Провести стандартизацию?'.format(path2))\n",
    "        if str(input()) != '0':\n",
    "            print('\\n{}:\\n'.format(path2))\n",
    "            standard_cnns(path2)\n",
    "            print('-----------')\n",
    "        else:\n",
    "            raise Exception('Невозможно объединить файлы нестандартного вида')\n",
    "\n",
    "    with open(path1, 'r') as file:\n",
    "        tmp1 = list(filter(bool, file.read().split('--')))\n",
    "        for i in range(len(tmp1)):\n",
    "            tmp1[i] = tmp1[i].split('ConvNetwork')\n",
    "            tmp1[i][0] = int(''.join(list(filter(lambda x: 48 <= ord(x) <= 58, tmp1[i][0]))))\n",
    "    with open(path2, 'r') as file:\n",
    "        tmp2 = list(filter(bool, file.read().split('--')))\n",
    "        for i in range(len(tmp2)):\n",
    "            tmp2[i] = tmp2[i].split('ConvNetwork')\n",
    "            tmp2[i][0] = int(''.join(list(filter(lambda x: 48 <= ord(x) <= 58, tmp2[i][0]))))\n",
    "    cnns = [tmp1[i][1] for i in range(len(tmp1))]\n",
    "    print(\n",
    "        '\\nMerging:\\nНомера сетей в фале {} не изменились\\nНомера сетей в файле {} перешли в следующие:\\n'.format(path1,\n",
    "                                                                                                                  path2))\n",
    "    for i, cnn in tmp2:\n",
    "        if cnn not in cnns:\n",
    "            print('{} --> {}'.format(i, len(tmp1) + 1))\n",
    "            tmp1.append([len(tmp1) + 1, cnn])\n",
    "            cnns.append(cnn)\n",
    "    with open('CNNs.txt', 'w') as file:\n",
    "        for i in range(len(tmp1)):\n",
    "            file.write('-----------------------\\n' + str(tmp1[i][0]) + '\\nConvNetwork' + tmp1[i][1])\n",
    "\n",
    "\n",
    "def next_cnn(path):\n",
    "    \"\"\"\n",
    "    Вычисляет номер следующей нейросети\n",
    "    :param path (string): Путь к файлу с нейросетями\n",
    "    :return: Номер следующей нейросети\n",
    "    \"\"\"\n",
    "    try:\n",
    "        open(path, 'r')\n",
    "    except:\n",
    "        with open(path, 'w') as file:\n",
    "            file.write('')\n",
    "        return 1\n",
    "    if not check_standard_cnns(path):\n",
    "        print('Файл {} не приведён к стандартному виду. Провести стандартизацию?'.format(path))\n",
    "        if str(input()) != '0':\n",
    "            print('\\n{}:\\n'.format(path))\n",
    "            standard_cnns(path)\n",
    "            print('-----------')\n",
    "    with open(path, 'r') as file:\n",
    "        \n",
    "        tmp = list(filter(bool, file.read().split('--')))\n",
    "        return len(tmp) + 1\n"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "id": "hnL_rzmXcKG2",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def maps_dims(im_dims, conv_size, pool_size, conv_stride, pool_stride):\n",
    "    \"\"\"\n",
    "    Вычисление размеров карт признаков после свёрточных слоёв\n",
    "    :param im_dims (tuple, list, ndarray): Размеры входного изображения\n",
    "    :param conv_size (tuple, list, ndarray): Размер фильтра на каждом слое\n",
    "    :param pool_size (tuple, list, ndarray): Размер ядра пулинга на каждом слое\n",
    "    :param conv_stride (tuple, list, ndarray): Величина шага фильтра свёртки на каждом слое\n",
    "    :param pool_stride (tuple, list, ndarray): Величина шага ядра пулинга на каждом слое\n",
    "    :return: Размеры карт признаков после прохождения свёрточных слоёв\n",
    "    \"\"\"\n",
    "    h, w = im_dims\n",
    "    for i in range(len(conv_size)):\n",
    "        h = (h - conv_size[i]) // conv_stride[i] + 1\n",
    "        w = (w - conv_size[i]) // conv_stride[i] + 1\n",
    "        if pool_size[i]:\n",
    "            h = (h - pool_size[i]) // pool_stride[i] + 1\n",
    "            w = (w - pool_size[i]) // pool_stride[i] + 1\n",
    "    return (h, w)\n",
    "\n"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "id": "cmbISuERWRxC"
   },
   "cell_type": "code",
   "source": [
    "def get_added_picture(image, filter):\n",
    "    \"\"\"\n",
    "    Применяет к изображению фильтр (нужно для аугментации)\n",
    "    :param image: Изображение, которое нужно изменить\n",
    "    :param filter: Фильтр, который нужно применить\n",
    "    :return: Изменённое изображение\n",
    "    \"\"\"\n",
    "    return image.filter(filter) if filter else image\n",
    "\n",
    "\n",
    "def pic_num(path):\n",
    "    \"\"\"\n",
    "    Функция вычисляет номер изображения в файле типа \"<путь>/pic_<номер>.jpg\"\n",
    "    :param path: Путь к изображению\n",
    "    :return: Номер изображения\n",
    "    \"\"\"\n",
    "    return int(str(path).split('pic_')[1][:4])\n"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "id": "Cs7kvCqhWRxF"
   },
   "cell_type": "markdown",
   "source": [
    "Чтобы не перегружать класс нейросети ненужными строками кода, все слои разобьём на несколько составляющих и будем держать их в списках, объединённых в один большой словарь. Его мы передаём в класс нейросети, в котором все эти составляющие будут поочерёдно применяться ко входным данным.\n",
    "\n",
    "Незначащие параметры вынесем в функции, генерирующие списки слоёв этих параметров"
   ]
  },
  {
   "metadata": {
    "id": "sr-ewGIhcLQZ",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def conv_addition(input, hn, hl, conv_size, conv_stride):\n",
    "    \"\"\"\n",
    "    Создание свёрточных слоёв по заданным параметрам\n",
    "    :param input (int): Количество каналов во входном изображении\n",
    "    :param hn (list, tuple, ndarray): Количество фильтров на каждом слое\n",
    "    :param hl (int): Количество слоёв\n",
    "    :param conv_size (list, tuple, ndarray): Размер фильтра на каждом слое\n",
    "    :param conv_stride (list, tuple, ndarray): Величина шага фильтра свёртки на каждом слое\n",
    "    :return: Свёрточные слои нейросети (list)\n",
    "    \"\"\"\n",
    "    convolutions = \\\n",
    "        [\n",
    "            torch.nn.Conv2d(in_channels=input, out_channels=hn[0],\n",
    "                            kernel_size=conv_size[0],\n",
    "                            stride=conv_stride[0])\n",
    "        ]  # Объявление свёртки для первого слоя\n",
    "    for i in range(hl - 1):  # Объяление свёрток для остальных слоёв\n",
    "        convolutions.append(\n",
    "            torch.nn.Conv2d(in_channels=hn[i], out_channels=hn[i + 1],\n",
    "                            kernel_size=conv_size[i + 1], stride=conv_stride[i + 1]))\n",
    "    return convolutions\n",
    "        \n",
    "    \n",
    "def pool_addition(hl, pool_size, pool_stride):\n",
    "    \"\"\"\n",
    "    Создание слоёв пулинга по заданным параметрам\n",
    "    :param hl (int): Количество слоёв\n",
    "    :param pool_size (list, tuple, ndarray): Размер ядра пулинга на каждом слое\n",
    "    :param pool_stride (list, tuple, ndarray): Величина шага ядра пулинга на каждом слое\n",
    "    :return: Пулинг-слои нейронной сети (list)\n",
    "    \"\"\"\n",
    "    pool = []\n",
    "    for i in range(hl): \n",
    "        if pool_size[i]:\n",
    "            pool.append(torch.nn.MaxPool2d(kernel_size=pool_size[i], stride=pool_stride[i]))\n",
    "        else:\n",
    "            pool.append(None)\n",
    "    return pool\n",
    "    \n",
    "    \n",
    "def linears_addition(input, hn, hl, output):\n",
    "    \"\"\"\n",
    "    Создание линейных слоёв по заданным параметрам\n",
    "    :param input (int): Количество input-нейронов\n",
    "    :param hn (list, tuple, ndarray): Количество нейронов на каждом слое\n",
    "    :param hl (int): Количество полносвязных слоёв\n",
    "    :param output (int): Количество output-нейронов\n",
    "    :return: Линейные слои нейронной сети (list)\n",
    "    \"\"\"\n",
    "    linears = []\n",
    "    linears = [torch.nn.Linear(input, hn[0])]\n",
    "    for i in range(hl - 1):\n",
    "        linears.append(\n",
    "            torch.nn.Linear(hn[i], hn[i + 1]))\n",
    "    linears.append(torch.nn.Linear(hn[-1], output))\n",
    "    return linears\n"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "id": "qsatNfz_cM3i",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def draw_plot(path, number, size=None, color='r'):\n",
    "    \"\"\"\n",
    "    Отрисовывает график losses одной нейросети, применяя нелинейную регрессию\n",
    "    :param path (string): Путь к файлу\n",
    "    :param number (int, string): Номер нейросети для отрисовки\n",
    "    :param size (int): Предпологаемый размер losses\n",
    "    :param color (string): Цвет для отрисовки\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not size:\n",
    "        size = len(get_loss(path, number))\n",
    "    \n",
    "    x = np.arange(size)\n",
    "    y = np.array(get_loss(path, number))\n",
    "\n",
    "    x = x[:, np.newaxis]\n",
    "    y = y[:, np.newaxis]\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=7)\n",
    "    x_poly = polynomial_features.fit_transform(x)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_poly, y)\n",
    "    y_poly_pred = model.predict(x_poly)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_poly_pred))\n",
    "    r2 = r2_score(y, y_poly_pred)\n",
    "\n",
    "    sort_axis = operator.itemgetter(0)\n",
    "    sorted_zip = sorted(zip(x, y_poly_pred), key=sort_axis)\n",
    "    x, y_poly_pred = zip(*sorted_zip)\n",
    "    plt.plot(x, y_poly_pred, color=color, label=str(number))\n",
    "\n",
    "\n",
    "def show_losses(path, numbers, size, colors):\n",
    "    \"\"\"\n",
    "    Отрисовывает графики losses указанных нейросетей\n",
    "    :param numbers (list, tuple): Номера сетей для отрисовки\n",
    "    :param size (int): Предпологаемый размер losses\n",
    "    :param colors (list, tuple): Набор цветов для отрисовки\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, j in enumerate(numbers):\n",
    "        draw_plot(path, j, size, colors[i])\n",
    "    plt.legend(numbers)\n"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "id": "hpbVG4cpWRxK"
   },
   "cell_type": "code",
   "source": [
    "def train(train_params):\n",
    "    \"\"\"\n",
    "    Обучает нейросеть на обучающей выборке\n",
    "    :param train_params (dict): Все параметры для обучения нейросети\n",
    "    :return: Обученная нейросеть\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = train_params['primary_batch']\n",
    "    trainloader = DataLoader(train_params['trainset'], batch_size=batch_size, shuffle=True)\n",
    "    batch_decrease = train_params['batch_decrease']\n",
    "    net = train_params['primary_net']\n",
    "    num_epochs = train_params['num_epochs']\n",
    "    write_seq = train_params['write_seq']\n",
    "    show_seq = train_params['show_seq']\n",
    "    lr = train_params['lr'] if train_params['lr'] else 0.001\n",
    "    lr_decrease = train_params['lr_decrease'] if train_params['lr_decrease'] else 1\n",
    "    loss_fn = train_params['loss_fn']\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    if net.params['new'] and net.params['write']:\n",
    "        f = open('Losses.txt', 'a')\n",
    "        f.write('&{}'.format(net.params['name']) + '\\n' + '[')\n",
    "        f.close()\n",
    "        net.params['new'] = False\n",
    "\n",
    "    learning_rate = lr\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "\n",
    "    current_loss = 0.0\n",
    "    accuracy = []\n",
    "\n",
    "    for epoch in tqdm_notebook(range(num_epochs)):\n",
    "        f = open('Losses.txt', 'a')\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(tqdm_notebook(trainloader)):\n",
    "            X_train, y_train = batch\n",
    "            try:\n",
    "                X_train, y_train = X_train.cuda(), y_train.cuda()\n",
    "            except:\n",
    "                pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = net(X_train)\n",
    "            loss = loss_fn(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            current_loss += loss.item()\n",
    "\n",
    "            if i % write_seq == write_seq - 1:\n",
    "                losses.append(current_loss / write_seq)\n",
    "                current_loss = 0.0\n",
    "\n",
    "            if i % show_seq == show_seq - 1:\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / show_seq))\n",
    "                running_loss = 0.0\n",
    "\n",
    "                if net.params['write']:\n",
    "                    torch.save(net, 'CNN{}'.format(net.params['name']))\n",
    "\n",
    "        learning_rate /= lr_decrease\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "        if epoch % 2:\n",
    "            batch_size = int(batch_size // batch_decrease)\n",
    "            batch_size = max(4, batch_size)\n",
    "            trainloader = DataLoader(train_params['trainset'], batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "        np.array(losses).round(3)\n",
    "        if net.params['write']:\n",
    "            for i in losses:\n",
    "                f.write(str(i) + ' ')\n",
    "            losses = []\n",
    "            f.write(']\\n' if epoch + 1 == num_epochs else ' ')\n",
    "            f.close()\n",
    "\n",
    "    print('Обучение закончено')\n",
    "    return net\n"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "id": "i7_5fxYpcOJN",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def validation(val_params):\n",
    "    \"\"\"\n",
    "    Валидирует нейросеть на предложенном датасете\n",
    "    :param val_params (dict): Все параметры для валидации \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    net = val_params['net']\n",
    "    batch_size = val_params['batch_size']\n",
    "    valloader = DataLoader(val_params['valset'], batch_size=batch_size, shuffle=False)\n",
    "    loss_fn = val_params['loss_fn']\n",
    "    \n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    running_loss = 0.0\n",
    "    total = valloader.__len__() * valloader.batch_size\n",
    "    for X_val, y_val in tqdm_notebook(valloader):\n",
    "        \n",
    "        X_val, y_val = X_val.cuda(), y_val.cuda()\n",
    "        \n",
    "        y_pred = net(X_val)\n",
    " \n",
    "        loss = loss_fn(y_pred, y_val).data\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    " \n",
    "        correct += (predicted == y_val).sum().item()\n",
    "        running_loss += loss\n",
    " \n",
    "    print(\"Validation Loss: {}\".format(running_loss / len(valloader)))\n",
    "    print(correct / total * 100, '%')\n",
    " \n",
    "    if net.params['write']:\n",
    "        with open('CNNs.txt', 'a') as file:\n",
    "            file.write('--------------------------------\\n' + str(net.params['name']) + '\\n' + str(net) + '\\n\\n')\n",
    "            file.write(\"Validation Loss: {}\".format(running_loss / len(valloader)) + '\\n')\n",
    "            file.write(\"Total accuracy: %d %%\" % (correct / total * 100) + '\\n')"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def test(test_params):\n",
    "    \"\"\"\n",
    "    :param test_params (dict): Все параметры для тестирования нейросети\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    testset = test_params['testset']\n",
    "    batch_size = test_params['batch_size']\n",
    "    net = test_params['net']\n",
    "    net.eval()\n",
    "    \n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    with open('submission.csv', 'w') as file:\n",
    "        file.write('ImageId,IdCls\\n')\n",
    "        for i, X_test in enumerate(testloader):\n",
    "            X_test = X_test.cuda()\n",
    "            y_pred = net(X_test)\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            \n",
    "            for j, pred in enumerate(predicted):\n",
    "                file.write(str(testloader.dataset.files[i * batch_size + j]).split('/')[-1] + ',' + str(pred.item()) + '\\n')\n",
    "    print('Тестирование завершено, результаты сохранены')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "id": "gz1IAx3MWRxR"
   },
   "cell_type": "code",
   "source": [
    "def show_accuracy(valloader, net):\n",
    "    y_val_all = torch.Tensor().long()\n",
    "    predictions_all = torch.Tensor().long()\n",
    "\n",
    "    # Пройдём по всему validation датасету и запишем ответы сети\n",
    "    with torch.no_grad():\n",
    "        for X, y in valloader:\n",
    "            predictions = net(X)\n",
    "            y_val = y\n",
    "            _, predictions = torch.max(predictions, 1)\n",
    "\n",
    "            # Аналог append для list\n",
    "            y_val_all = torch.cat((y_val_all, y_val), 0)\n",
    "            predictions_all = torch.cat((predictions_all, predictions), 0)\n",
    "\n",
    "    class_correct = [0 for i in range(19)]\n",
    "    class_total = [0 for i in range(19)]\n",
    "\n",
    "    c = (predictions_all == y_val_all).squeeze()\n",
    "    for i in range(len(predictions_all)):\n",
    "        label = predictions_all[i]\n",
    "        class_correct[label] += c[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "    print(class_total)\n",
    "\n",
    "    for i in range(len(valloader.dataset.labels)):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            (dataset.labels[i], (100 * class_correct[i] / class_total[i]) if class_total[i] != 0 else -1)))"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "id": "_-DtnO8EcQkc",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class ConvNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, p):\n",
    "        \"\"\"\n",
    "        Инициализация нейронной сети\n",
    "        :param p (dict): словарь параметров для нейронной сети\n",
    "        \"\"\"\n",
    "        super(ConvNetwork, self).__init__()\n",
    "\n",
    "        if len(p['fc']['drop_out']) != p['fc']['hl']:  # дополнение раздела 'drop_out' в словаре\n",
    "            i = len(p['fc']['drop_out'])\n",
    "            for _ in range(i, p['fc']['hl']):\n",
    "                p['fc']['drop_out'].append(p['fc']['drop_out'][i - 1])\n",
    "\n",
    "        if len(p['conv']['drop_out']) != p['conv']['hl']:\n",
    "            i = len(p['conv']['drop_out'])\n",
    "            for _ in range(i, p['conv']['hl']):\n",
    "                p['conv']['drop_out'].append(p['conv']['drop_out'][i - 1])\n",
    "        \n",
    "        for n in ('conv', 'fc'):\n",
    "            if len(p[n]['batch_norm']) != p[n]['hl']:  # дополнение раздела 'batch_norm' в словаре\n",
    "                i = len(p[n]['batch_norm'])\n",
    "                for _ in range(i, p[n]['hl']):\n",
    "                    p[n]['batch_norm'].append(False)\n",
    "            for i in range(p[n]['hl']):\n",
    "                if p[n]['batch_norm'][i]:\n",
    "                    p[n]['batch_norm'][i] = torch.nn.BatchNorm2d(p[n]['hn'][i]) if n == 'conv' else torch.nn.BatchNorm1d(p[n]['hn'][i])\n",
    "                else:\n",
    "                    p[n]['batch_norm'][i] = None\n",
    "         \n",
    "        self.params = p\n",
    "\n",
    "    def convolution_step(self, x):\n",
    "        \"\"\"\n",
    "        Обработка входной информации свёрточными слоями (input -> convolution)\n",
    "        :param x (Tensor): Входные данные: матрица признаков объекта\n",
    "        :return: Вектор карт признаков, полученных в результате свёртки и пулинга\n",
    "        \"\"\"\n",
    "        for i in range(self.params['conv']['hl']):\n",
    "            x = self.params['conv']['convolutions'][i](x)  # Свёртка \n",
    "            if self.params['conv']['activations'][i]:\n",
    "                x = self.params['conv']['activations'][i](x)   # Пропуск через функцию активации (если указана)\n",
    "            if self.params['conv']['drop_out'][i]:  # Обнуление некоторых нейронов (drop_out, если указан)\n",
    "                x = self.params['conv']['drop_out'][i](x)\n",
    "            if self.params['conv']['batch_norm'][i]:  # Нормализация батчей (если указана)\n",
    "                x = self.params['conv']['batch_norm'][i](x)\n",
    "            if self.params['conv']['pool_size'][i]:\n",
    "                x = self.params['conv']['pool'][i](x)  # Пулинг получившихся карт (если указан)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def fc_step(self, x):\n",
    "        \"\"\"\n",
    "        Обработка входной информации полносвязными слоями (convolution -> fc)\n",
    "        :param x (Tensor): Выходные данные для слоёв свёртки\n",
    "        :return: Вероятность принадлежности к каждому классу\n",
    "        \"\"\"\n",
    "        x = x.view(-1, self.params['fc']['input'])  # Представление входных даных в виде батчей векторов\n",
    "        for i in range(self.params['fc']['hl']):\n",
    "            x = self.params['fc']['linears'][i](x)  # Пропуск через линейный слой\n",
    "            if self.params['fc']['activations'][i]:\n",
    "                x = self.params['fc']['activations'][i](x)  # Пропуск через функцию активации (если указана)\n",
    "            if self.params['fc']['drop_out'][i]:\n",
    "                x = self.params['fc']['drop_out'][i](x)  # Обнуление некоторых нейронов (drop_out, если указан)\n",
    "            if self.params['fc']['batch_norm'][i]:\n",
    "                x = self.params['fc']['batch_norm'][i](x)  # Нормализация батчей (если указана)\n",
    "        return self.params['fc']['linears'][-1](x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Обработка входных данных нейронной сетью\n",
    "        :param x (Tensor): Входные данные: матрица признаков объекта\n",
    "        :return: Вероятность принадлежности к каждому классу\n",
    "        \"\"\"\n",
    "        return self.fc_step(self.convolution_step(x))\n"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "id": "AGqGtsiAWRxW",
    "outputId": "d7fdca3f-904f-4fb5-c1d3-58308a8b0460"
   },
   "cell_type": "code",
   "source": [
    "csv_file = pd.read_csv('../input/almaz-antey-hackathon-l0/train_classification.csv', index_col=0)"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "id": "IQoP2_BkWRxZ"
   },
   "cell_type": "code",
   "source": [
    "path = '../input/almaz-antey-hackathon-l0/train/train/'\n",
    "train_files_set = sorted(list(Path(path).rglob('*jpg')))\n",
    "test_files = sorted(list(Path('../input/almaz-antey-hackathon-l0/test/test/').rglob('*jpg')))"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "picture_size = (768, 768)"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "trainset = ShipsDataset(files=train_files_set, mode='train', transform=transforms.Compose([\n",
    "    transforms.Resize(picture_size),\n",
    "    transforms.ToTensor()\n",
    "]), csv_file=csv_file)\n",
    "\n",
    "testset = ShipsDataset(files=test_files, mode='test', transform=transforms.Compose([\n",
    "    transforms.Resize(picture_size),\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "valset = None"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "testloader = DataLoader(testset, batch_size=8, shuffle=False)"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "id": "BeJdP38QWRxf"
   },
   "cell_type": "markdown",
   "source": [
    "Здесь назначаем все параметры для сети, тренировки, валидации и тестирования"
   ]
  },
  {
   "metadata": {
    "id": "3HOLzQo3cUFT",
    "trusted": true,
    "outputId": "111a4409-b481-4383-c617-e4c545e85a45"
   },
   "cell_type": "code",
   "source": [
    "model_params = dict()\n",
    "model_params['conv'] = dict()\n",
    "model_params['fc'] = dict()\n",
    "model_params['name'] = str(next_cnn('CNNs.txt'))  # Имя нейронной сети (использовать номера)\n",
    "model_params['new'] = True  # Новая сеть или нет. Если новая, будет совершена попытка загрузить данные из файла \"CNN<name>.txt\"\n",
    "model_params['write'] = True  # Нужно ли записывать данные при обучении нейросети \n",
    "\n",
    "model_params['conv']['input']        = 1  # Количество каналов во входном изображении\n",
    "model_params['conv']['hn']           = [64, 64, 32, 64, 64]  # Количество фильтров на каждом слое\n",
    "model_params['conv']['hl']           = len(model_params['conv']['hn'])  # Количество слоёв\n",
    "model_params['conv']['conv_size']    = [5, 7, 5, 5, 3]  # Размер фильтра на каждом слое\n",
    "model_params['conv']['pool_size']    = [3, 2, 3, 2, 3]  # Размер ядра пулинга на каждом слое\n",
    "model_params['conv']['conv_stride']  = [2, 1, 1, 1, 1]  # Величина шага фильтра свёртки на каждом слое (stride)\n",
    "model_params['conv']['pool_stride']  = [2, 2, 2, 2, 2]  # Величина шага ядра пулинга на каждом слое (stride)\n",
    "model_params['conv']['batch_norm']   = [True, False, True, False, True]  # Нормализация батча для каждого слоя \n",
    "model_params['conv']['convolutions'] = conv_addition(model_params['conv']['input'], model_params['conv']['hn'], model_params['conv']['hl'],\n",
    "                                                     model_params['conv']['conv_size'], model_params['conv']['conv_stride'])\n",
    "model_params['conv']['pool']         = pool_addition(model_params['conv']['hl'], model_params['conv']['pool_size'], \n",
    "                                                     model_params['conv']['pool_stride'])\n",
    "model_params['conv']['activations']  = [torch.nn.ReLU(), torch.nn.ReLU(), torch.nn.ReLU(), torch.nn.ReLU(), torch.nn.ReLU()]\n",
    "model_params['conv']['drop_out']     = [None, torch.nn.Dropout(), None, torch.nn.Dropout(), None]  # Указание Dropout для каждого слоя (автоматически дополняется)\n",
    "\n",
    "\n",
    "h, w = picture_size # Указание высоты и ширины входной картинки\n",
    "h, w = maps_dims([h, w], model_params['conv']['conv_size'], model_params['conv']['pool_size'], \n",
    "                 model_params['conv']['conv_stride'], model_params['conv']['pool_stride'])\n",
    "print('output map height: {};\\toutput map width: {};\\n'.format(h, w))\n",
    "print('model name: {}'.format(model_params['name']))\n",
    "\n",
    "\n",
    "model_params['fc']['input']       = h * w * model_params['conv']['hn'][-1]  # Количество входных данных (количество input-нейронов)\n",
    "model_params['fc']['output']      = 2  # Количество выходных данных (Количество output-нейронов)\n",
    "model_params['fc']['hn']          = [1000]# Количество нейронов на каждом слое\n",
    "model_params['fc']['hl']          = len(model_params['fc']['hn'])  # Количество полносвязных слоёв\n",
    "model_params['fc']['activations'] = [torch.nn.ReLU()]  # Объявление функций активации для каждого слоя\n",
    "model_params['fc']['linears']     = linears_addition(model_params['fc']['input'], model_params['fc']['hn'], \n",
    "                                                 model_params['fc']['hl'], model_params['fc']['output'])  # Объявление линейных слоёв\n",
    "model_params['fc']['drop_out']    = [torch.nn.Dropout(), torch.nn.Dropout()]  # Указание Dropout для каждого слоя (автоматически дополняется)\n",
    "model_params['fc']['batch_norm']  = [True, True]  # Указание BatchNormalization для каждого слоя (дополняется автоматически)\n"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "text": "output map height: 8;\toutput map width: 8;\n\nmodel name: 1\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {
    "id": "LlLmqwYEcVI3",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train_params                   = dict() \n",
    "train_params['lr']             = 0.0005  # первоначальный learning_rate\n",
    "train_params['lr_decrease']    = 2  # Уменьшение learning_rate после каждой эпохи. Если нет необходимости, выставить 1\n",
    "train_params['num_epochs']     = 4  # Количество эпох\n",
    "train_params['write_seq']      = 50  # Частота записи лоссов обучаемой нейросети\n",
    "train_params['show_seq']       = 20  # Частота отображения лоссов обучаемой нейросети\n",
    "train_params['loss_fn']        = torch.nn.CrossEntropyLoss()  # Функция потерь\n",
    "train_params['trainset']    = trainset  # Датасет для тренировки\n",
    "train_params['testset']     = testset\n",
    "train_params['primary_net']    = None  # Изначальная сеть\n",
    "train_params['final_net']      = None  # Обученная сеть\n",
    "train_params['losses_path']    = 'Losses.txt'  # Путь для сохранения лоссов (в kaggle kernels приходится дописывать полный путь в функциях)\n",
    "train_params['networks_path']  = 'CNNs.txt'  # Путь для сохранения данных о нейросети\n",
    "train_params['primary_batch']  = 64  # Изначальный размер батчей\n",
    "train_params['batch_decrease'] = 2  # Уменьшение размера батчей каждые две эпохи. Если нет необходимости, выставить 1\n"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "id": "BTlr_LXtWRxk"
   },
   "cell_type": "code",
   "source": [
    "val_params = dict()\n",
    "val_params['batch_size'] = 16  # Размер батчей\n",
    "val_params['valset'] = valset  # Датасет для валидации\n",
    "val_params['loss_fn'] = torch.nn.functional.cross_entropy  # Функция потерь (При использовании класса в torch.nn по необъяснимым причинам\n",
    "# 16 GB RAM уничтожаются за несколько секунд, поэтому используется данный вариант)\n",
    "val_params['net'] = trainset  # Нейросеть для валидации"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "id": "KKZDlqJeWRxm"
   },
   "cell_type": "code",
   "source": [
    "test_params = dict()\n",
    "test_params['testset'] = testset  # Датасет для тестирования\n",
    "test_params['batch_size'] = 16  # Размер батчей\n",
    "test_params['net'] = None  # Нейросеть для тестирования"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {
    "id": "u8oc2DroWRxn"
   },
   "cell_type": "markdown",
   "source": [
    "Так как мы записывали все слои в виде словаря и в классе нейросети они будут храниться в виде списков, из которых просто будет извлекаться информация о конкретном слое, то параметры сети остаются пустыми. Это приводит к тому, что torch.nn.module просто не понимает, что мы от него хотим и выдаёт ошибку или не обучается (потому что обучаться нечему, как он считает).\n",
    "\n",
    "Поэтому все параметры придётся назначать вручную. Написав данный код один раз, про него можно забыть"
   ]
  },
  {
   "metadata": {
    "id": "9QfR4uE9cV7J",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "if not model_params['new']: \n",
    "    try:\n",
    "        train_params['primary_net'] = torch.load('../input/kernel0fc93dd4ed/CNN{}'.format(model_params['name']))\n",
    "    except:\n",
    "        model_params['new'] = True\n",
    "if model_params['new']:\n",
    "    train_params['primary_net'] = ConvNetwork(model_params)  # Инициализация сети\n",
    "\n",
    "    for i in range(model_params['conv']['hl']):  # Инициализация параметров свёрточных слоёв сети\n",
    "        train_params['primary_net'].__setattr__('convolution_{}'.format(i + 1), model_params['conv']['convolutions'][i])\n",
    "        if model_params['conv']['activations'][i]:\n",
    "            train_params['primary_net'].__setattr__('conv_activation_{}'.format(i + 1), model_params['conv']['activations'][i])\n",
    "        if model_params['conv']['batch_norm'][i]:\n",
    "            train_params['primary_net'].__setattr__('conv_batch_norm_{}'.format(i + 1), model_params['conv']['batch_norm'][i])\n",
    "        if model_params['conv']['pool_size'][i]:\n",
    "            train_params['primary_net'].__setattr__('pool_{}'.format(i + 1), model_params['conv']['pool'][i])\n",
    "\n",
    "    for i in range(model_params['fc']['hl']):  # Инициализация параметров полносвязных слоёв сети\n",
    "        train_params['primary_net'].__setattr__('linear_{}'.format(i + 1), model_params['fc']['linears'][i])\n",
    "        if model_params['fc']['activations'][i]:\n",
    "            train_params['primary_net'].__setattr__('fc_activation_{}'.format(i + 1), model_params['fc']['activations'][i])\n",
    "        if model_params['fc']['drop_out'][i]:\n",
    "            train_params['primary_net'].__setattr__('drop_{}'.format(i + 1), model_params['fc']['drop_out'][i])\n",
    "        if model_params['fc']['batch_norm'][i]:\n",
    "            train_params['primary_net'].__setattr__('fc_batch_norm_{}'.format(i + 1), model_params['fc']['batch_norm'][i])\n",
    "    train_params['primary_net'].__setattr__('linear_{}'.format(model_params['fc']['hl'] + 1), model_params['fc']['linears'][-1])\n",
    "\n"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "id": "zM6vk5tVcXeo",
    "trusted": true,
    "outputId": "ccb32149-8804-4a79-f07a-92ae4ab3730c"
   },
   "cell_type": "code",
   "source": [
    "train_params['primary_net'].cuda()\n",
    "print(train_params['primary_net'])"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "text": "ConvNetwork(\n  (convolution_1): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2))\n  (conv_activation_1): ReLU()\n  (conv_batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool_1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (convolution_2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1))\n  (conv_activation_2): ReLU()\n  (pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (convolution_3): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1))\n  (conv_activation_3): ReLU()\n  (conv_batch_norm_3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool_3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (convolution_4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n  (conv_activation_4): ReLU()\n  (pool_4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (convolution_5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n  (conv_activation_5): ReLU()\n  (conv_batch_norm_5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool_5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (linear_1): Linear(in_features=4096, out_features=1000, bias=True)\n  (fc_activation_1): ReLU()\n  (drop_1): Dropout(p=0.5, inplace=False)\n  (fc_batch_norm_1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (linear_2): Linear(in_features=1000, out_features=2, bias=True)\n)\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {
    "id": "5U2Ic-PncYEg",
    "trusted": true,
    "outputId": "fc54df9d-70ea-4285-efdc-7a9c9a020e8c"
   },
   "cell_type": "code",
   "source": [
    "\"Обучение нейросети\"\n",
    "train_params['final_net'] = train(train_params) if model_params['new'] else train_params['primary_net']\n",
    "val_params['net'], test_params['net'] = train_params['final_net'], train_params['final_net']\n"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "text": "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
     "name": "stderr"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed4d7aaf18f24fde9174d3e3a3c2e387"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:39: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
     "name": "stderr"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2250657c98054f658d9b6896990da1d9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "[1,    20] loss: 0.637\n[1,    40] loss: 0.615\n[1,    60] loss: 0.609\n[1,    80] loss: 0.499\n[1,   100] loss: 0.485\n[1,   120] loss: 0.487\n[1,   140] loss: 0.494\n[1,   160] loss: 0.472\n[1,   180] loss: 0.456\n[1,   200] loss: 0.432\n\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f82fcc8090042ee846657ea51579920"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "[2,    20] loss: 0.443\n[2,    40] loss: 0.398\n[2,    60] loss: 0.395\n[2,    80] loss: 0.341\n[2,   100] loss: 0.363\n[2,   120] loss: 0.386\n[2,   140] loss: 0.415\n[2,   160] loss: 0.329\n[2,   180] loss: 0.336\n[2,   200] loss: 0.383\n\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=407.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7b23e076e30495c901b5d1088488bb2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "[3,    20] loss: 0.348\n[3,    40] loss: 0.313\n[3,    60] loss: 0.382\n[3,    80] loss: 0.340\n[3,   100] loss: 0.373\n[3,   120] loss: 0.362\n[3,   140] loss: 0.350\n[3,   160] loss: 0.347\n[3,   180] loss: 0.366\n[3,   200] loss: 0.351\n[3,   220] loss: 0.320\n[3,   240] loss: 0.328\n[3,   260] loss: 0.317\n[3,   280] loss: 0.281\n[3,   300] loss: 0.312\n[3,   320] loss: 0.308\n[3,   340] loss: 0.329\n[3,   360] loss: 0.339\n[3,   380] loss: 0.295\n[3,   400] loss: 0.293\n\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=407.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03d66258b562463fa226e632b3d4bc66"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "[4,    20] loss: 0.304\n[4,    40] loss: 0.354\n[4,    60] loss: 0.335\n[4,    80] loss: 0.315\n[4,   100] loss: 0.299\n[4,   120] loss: 0.320\n[4,   140] loss: 0.293\n[4,   160] loss: 0.319\n[4,   180] loss: 0.327\n[4,   200] loss: 0.312\n[4,   220] loss: 0.288\n[4,   240] loss: 0.286\n[4,   260] loss: 0.311\n[4,   280] loss: 0.262\n[4,   300] loss: 0.276\n[4,   320] loss: 0.305\n[4,   340] loss: 0.259\n[4,   360] loss: 0.286\n[4,   380] loss: 0.283\n[4,   400] loss: 0.292\n\n\nОбучение закончено\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "id": "F_qdxEuNWRxu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "test(test_params)"
   ],
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Тестирование завершено, результаты сохранены\n",
     "name": "stdout"
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}